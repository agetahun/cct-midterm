# cct-midterm
Cultural Consensus Theory with PyMC - Midterm Assignment

I created a Cultural Consensus Theory (CCT) model using PyMC. The goal of the model is to analyze a dataset about local plant knowledge and estimate the consensus answers for questions about the local plants, as well as estimate the competence level of each informant. In this model, the latent variables "D" (informant competence) and "Z" (consensus answer) were used to determine the likelihood for each informant's answer. Each response is more likely to match the consensus answer if the informant is more competent, and less likely (closer to guessing) if the informant is less competent. I chose a Uniform Distribution to determine each informant's competence (Di) because there wasn't any prior information about the informant's competence level which means that any competence between 0.5 (completely guessing) and 1 (being very knowledgeable) is equally likely. I originally thought that using a Beta(2,2) distribution while favoring values near 0.5 would be better for a general population because it would assume that everyone has a moderate competence and would exclude any extremes (such as experts or people who know nothing about the local plants).
    This was written as: 
        raw_D = pm.Beta("raw_D", 2, 2, shape = rows) 
        D = pm.Deterministic("D", 0.5 + 0.5 * raw_D). 
However, I decided against using a Beta distribution for a more neutral one since I donâ€™t know anything about who the informants are and have no reason to assume that their knowledge would be biased towards a particular competence level. 

Posterior inference was performed using PyMC's MCMC sampler with 2000 samples across 4 chains for 1_000 tune. My analyze results function shows that the r_hat values are all 1.0, meaning that there was good convergence. The posterior distributions for competence showed that informant 6 (written as D[5]) was the most competent (mean Di of 0.883) and informant 3 (written as D[2]) was the least competent and used almost completely random guessing (mean Di of 0.565). When calculating the posterior mean probability for each consensus answer, the results showed that most questions had a clear agreed upon answer (either having a mean near 1 for a consensus on True or a mean near 0 for a consensus on False). The 6th question (written as Z[5]) had a mean Zj of 0.589, meaning that it was likely a difficult/ ambiguous question as there was no clear agreement on either True or False. The compute_majority_vote function calculates the simple majority vote and determines whether the majority of informants decided the answer was True or False based ONLY on the given data (each informant has equal weight). The compare_with_majority function compares this simple majority to the consensus key that my CCT model estimates, which weighs each informant's decision by their competence level before using all the answers to decide what the true answer should be. While most questions have the same correct answer, there are differences (questions: 3, 7, 9, 11, and 15) due to the CCT model's advantage of being able to outweigh the less knowledgeable informants.